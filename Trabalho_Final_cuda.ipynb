{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9YXUYLIneC3olvdjkO53M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulioBurlamaqui/MachineLearning/blob/main/Trabalho_Final_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "oCKpmgTjB_Vu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8c236f-e24c-48d6-f235-fa1042d9df5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim, no_grad\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from matplotlib.pyplot import imshow\n",
        "from time import time\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "tN_Zvv0dCh4x"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoF6JsVvNJ1d",
        "outputId": "3da1dd85-e963-4664-a53b-072924d5510f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importando dados"
      ],
      "metadata": {
        "id": "DfWL2GQzycfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST_treino = datasets.MNIST('./', train = True, transform = transforms.ToTensor(), download=True)\n",
        "MNIST_teste =  datasets.MNIST('./', train = False, transform = transforms.ToTensor(), download=False)"
      ],
      "metadata": {
        "id": "o4W_59-TESk_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(MNIST_treino), len(MNIST_teste))"
      ],
      "metadata": {
        "id": "HVZJvV7QFco3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e2f4d2-9619-447d-985f-eb0c48194628"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST = ConcatDataset((MNIST_treino, MNIST_teste))\n",
        "MNIST"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_THat_QymdI",
        "outputId": "83cbc99e-b7c3-4375-ba9a-a8a5589243c3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.ConcatDataset at 0x7f018d2a90c0>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = [label for _, label in MNIST]\n",
        "target"
      ],
      "metadata": {
        "id": "Te6rb9PV8i-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(MNIST_treino)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vYJ5f04zOWu",
        "outputId": "a7485a5d-d43e-45b5-d76f-bd4a5210ffd9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.mnist.MNIST"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  # Explorando o conjunto de dados"
      ],
      "metadata": {
        "id": "kC8vDFtTRlt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(MNIST)"
      ],
      "metadata": {
        "id": "Htmliv9hGjfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b839f585-78cb-4344-ba34-f34514fb32e1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.utils.data.dataset.ConcatDataset"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST[0]"
      ],
      "metadata": {
        "id": "Vauu5MvDGsW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad43d7c-8f37-4c71-a380-039a59357667"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
              "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
              "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
              "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
              "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
              "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
              "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
              "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
              "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
              "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
              "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
              "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
              "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
              "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
              "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
              "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(MNIST[0])"
      ],
      "metadata": {
        "id": "7sDahqwTGp6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d080680-6fa0-4459-f5e6-075316ce5359"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(MNIST[0][0])"
      ],
      "metadata": {
        "id": "2igOKoOVHBAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06085bd1-af24-4de7-a125-590af94dd7b2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(MNIST[0][0])"
      ],
      "metadata": {
        "id": "ROK9gEqaHDyT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d97b6122-fee6-46ca-98ba-a2f4cdfd298c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST[0][0]"
      ],
      "metadata": {
        "id": "uGRlP59BG7wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd80774-d0a0-4a9b-f632-a4f515b61201"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
              "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
              "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
              "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
              "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
              "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
              "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
              "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
              "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
              "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
              "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
              "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
              "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
              "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
              "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
              "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
              "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
              "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
              "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST[0][0][0].shape"
      ],
      "metadata": {
        "id": "8vPDD_jzXMtb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a2b8f6-6ebc-4c20-e9e9-866d68ab6a24"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST[0][0][0].shape[0]"
      ],
      "metadata": {
        "id": "Ir5FJ1zqXIxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06efb68b-1ecd-4a81-ff34-6f18d71a9449"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST[0][0][0].shape[0] * MNIST[0][0][0].shape[1]"
      ],
      "metadata": {
        "id": "CP1j0RDxHHx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb3807f-1950-4dc4-de19-683509312c40"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imagem, rotulo = MNIST_treino[11111]\n",
        "\n",
        "imshow(imagem[0])\n",
        "print(rotulo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "wna58216GYmr",
        "outputId": "e1026587-103d-4af4-d28d-c4e0f7233808"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hiperparâmetros"
      ],
      "metadata": {
        "id": "8sSfINqaGgKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'batch_size':       10,    #quantidade de amostras por iteração\n",
        "    'num_threads':      2,    #número de threads do DataLoader\n",
        "    #número real de batches = 20 (tamanho de cada batch * quantidade de threads)\n",
        "    'num_classes':      len(MNIST_teste.classes),\n",
        "    'taxa_aprendizado': 1e-4, # Encontrado euristicamente, hiperparâmetro\n",
        "    'weight_decay':     5e-4, # Encontrado euristicamente, hiperparâmetro\n",
        "    'num_epochs':       30,    # Encontrado euristicamente, hiperparâmetro\n",
        "    'folds':            5     # Quantidade de folds da validação cruzada\n",
        "}\n",
        "neurons = {\n",
        "    'input_size':   1 * 28 * 28, #dimensão de entrada (imagens de 28x28 bits com 1 canal de cor. precisa ser achatado para uma única dimensão)\n",
        "    'hidden_size':  128, # dimensão escondida, hiperparâmetro\n",
        "    'out_size':     args['num_classes']\n",
        "}"
      ],
      "metadata": {
        "id": "QAJuD0uXFTYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo funções da rede"
      ],
      "metadata": {
        "id": "O0ZmZZotGtU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_MLP(nn.Module):\n",
        "  #Definição da nossa própria multi-layered perceptron\n",
        "  def __init__(self, input_size, hidden_size, out_size):\n",
        "    super(MNIST_MLP, self).__init__()\n",
        "\n",
        "    #Definição das camadas e das funções de ativação intermediárias\n",
        "    # aqui escolhemos duas redes lineares e como funções de ativação, ReLU duas vezes\n",
        "    self.features  = nn.Sequential(\n",
        "                        nn.Linear(input_size, hidden_size),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(hidden_size, hidden_size),\n",
        "                        nn.ReLU())\n",
        "    #A de saída também será linear\n",
        "    self.out     = nn.Linear(hidden_size, out_size)\n",
        "    self.softmax = nn.Softmax()\n",
        "    #Como estamos lidando com um processo de classificação, passaremos por um softmax\n",
        "\n",
        "  def forward(self, input):\n",
        "    #Define a computação de cada chamada\n",
        "    input = input.view(input.size(0), -1) #Linearizando a entrada\n",
        "\n",
        "    feature = self.features(input)\n",
        "    output  = self.softmax(self.out(feature))\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "dSAazYaWF0D8"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def treina(train_set, rede, epoca):\n",
        "  rede.train() #modo treino\n",
        "\n",
        "  start = time()\n",
        "\n",
        "  epoch_loss = []\n",
        "  all_labels = []\n",
        "  all_preds = []\n",
        "\n",
        "  for batch in train_set:\n",
        "    dado, rotulo = batch\n",
        "\n",
        "    dado = dado.to('cuda')\n",
        "    rotulo = rotulo.to('cuda')\n",
        "\n",
        "    # Forward\n",
        "    pred = rede(dado)\n",
        "    loss = criterion(pred, rotulo)\n",
        "    epoch_loss.append(loss.cpu().item())\n",
        "\n",
        "    pred_labels = torch.argmax(pred, dim=1).cpu().numpy()\n",
        "    true_labels = rotulo.cpu().numpy()\n",
        "\n",
        "    all_labels.extend(true_labels)\n",
        "    all_preds.extend(pred_labels)\n",
        "\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "  end = time()\n",
        "\n",
        "  f1 = f1_score(all_labels, all_preds, average='micro')\n",
        "\n",
        "  print('\\n')\n",
        "  print(\"**********TREINA***********************\")\n",
        "  print(\"Época %d, Loss: %.4f +\\- %.4f, F1 Score: %.4f, Tempo: %.2f\" % (epoca, epoch_loss.mean(), epoch_loss.std(), f1, end-start))\n",
        "\n",
        "  return epoch_loss.mean()\n"
      ],
      "metadata": {
        "id": "DP73s473cvhM"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valida(teste_set, rede, epoca):\n",
        "  rede.eval() #modo avaliação\n",
        "\n",
        "  start = time()\n",
        "\n",
        "  epoch_loss = []\n",
        "  all_labels = []\n",
        "  all_preds = []\n",
        "\n",
        "  with no_grad():\n",
        "\n",
        "    for batch in teste_set:\n",
        "\n",
        "      dado, rotulo = batch\n",
        "\n",
        "      dado = dado.to('cuda')\n",
        "      rotulo = rotulo.to('cuda')\n",
        "\n",
        "      # Forward\n",
        "      pred = rede(dado)\n",
        "      loss = criterion(pred, rotulo)\n",
        "      epoch_loss.append(loss.cpu().item())\n",
        "\n",
        "      pred_labels = torch.argmax(pred, dim=1).cpu().numpy()\n",
        "      true_labels = rotulo.cpu().numpy()\n",
        "\n",
        "      all_labels.extend(true_labels)\n",
        "      all_preds.extend(pred_labels)\n",
        "\n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "\n",
        "  f1 = f1_score(all_labels, all_preds, average='micro')\n",
        "\n",
        "  end = time()\n",
        "\n",
        "  print(\"**********VALIDA********************\")\n",
        "  print(\"Época %d, Loss: %.4f +\\- %.4f, F1 Score: %.4f, Tempo: %.2f\" % (epoca, epoch_loss.mean(), epoch_loss.std(), f1, end-start))\n",
        "\n",
        "  return epoch_loss.mean()"
      ],
      "metadata": {
        "id": "UKyVYB-wbecP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def executa(treino, teste, rede, epocas):\n",
        "  treino_loss, teste_loss = [], []\n",
        "  for epoca in range(epocas):\n",
        "    # Train\n",
        "    treino_loss.append(treina(treino, rede, epoca))\n",
        "\n",
        "    # Validate\n",
        "    teste_loss.append(valida(teste, rede, epoca))\n",
        "\n",
        "  return treino_loss, teste_loss"
      ],
      "metadata": {
        "id": "D4v4gYJbcYAh"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando a rede"
      ],
      "metadata": {
        "id": "fw27JX1xw6Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rede = MNIST_MLP(neurons['input_size'], neurons['hidden_size'], neurons['out_size']).to('cuda')\n",
        "print(rede)"
      ],
      "metadata": {
        "id": "_ntsjRwNeGJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d7cbf2-5c6e-4331-fa63-768c0cd00c5a"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST_MLP(\n",
            "  (features): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (out): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=None)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to('cuda') #CrossEntropy indicado para classificação multiclasses"
      ],
      "metadata": {
        "id": "P2rNLzLdfLBD"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(rede.parameters(), lr=args['taxa_aprendizado'], weight_decay=args['weight_decay'])\n",
        "#Pesquisar sobre pq Adam (sei leigamente que é mt bom)"
      ],
      "metadata": {
        "id": "2XN1UeCMboGs"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execução"
      ],
      "metadata": {
        "id": "Fi6dZkavGzXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kfold = StratifiedKFold(n_splits=args['folds'], shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "-tqquqgxSkWa"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4grMaCP164C0",
        "outputId": "a4e76473-8d42-4607-97d3-c95ba4167be8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000,\n",
              "           0.6235, 0.9922, 0.6235, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1882, 0.9333,\n",
              "           0.9882, 0.9882, 0.9882, 0.9294, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118, 0.8902, 0.9922,\n",
              "           0.9882, 0.9373, 0.9137, 0.9882, 0.2235, 0.0235, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0392, 0.2353, 0.8784, 0.9882, 0.9922,\n",
              "           0.9882, 0.7922, 0.3294, 0.9882, 0.9922, 0.4784, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.6392, 0.9882, 0.9882, 0.9882, 0.9922,\n",
              "           0.9882, 0.9882, 0.3765, 0.7412, 0.9922, 0.6549, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.2000, 0.9333, 0.9922, 0.9922, 0.7451, 0.4471,\n",
              "           0.9922, 0.8941, 0.1843, 0.3098, 1.0000, 0.6588, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.1882, 0.9333, 0.9882, 0.9882, 0.7020, 0.0471, 0.2941,\n",
              "           0.4745, 0.0824, 0.0000, 0.0000, 0.9922, 0.9529, 0.1961, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.1490, 0.6471, 0.9922, 0.9137, 0.8157, 0.3294, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.9922, 0.9882, 0.6471, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275,\n",
              "           0.6980, 0.9882, 0.9412, 0.2784, 0.0745, 0.1098, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.9922, 0.9882, 0.7647, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2235,\n",
              "           0.9882, 0.9882, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.9922, 0.9882, 0.7647, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7765,\n",
              "           0.9922, 0.7451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9922, 0.7686, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2980, 0.9647,\n",
              "           0.9882, 0.4392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.9922, 0.9882, 0.5804, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.9882,\n",
              "           0.9020, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0275, 0.5294, 0.9922, 0.7294, 0.0471, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.9882,\n",
              "           0.8745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0275, 0.5137, 0.9882, 0.8824, 0.2784, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.9882,\n",
              "           0.5686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.1882, 0.6471, 0.9882, 0.6784, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3373, 0.9922,\n",
              "           0.8824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4471,\n",
              "           0.9333, 0.9922, 0.6353, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.9882,\n",
              "           0.9765, 0.5725, 0.1882, 0.1137, 0.3333, 0.6980, 0.8824, 0.9922,\n",
              "           0.8745, 0.6549, 0.2196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.9882,\n",
              "           0.9882, 0.9882, 0.8980, 0.8431, 0.9882, 0.9882, 0.9882, 0.7686,\n",
              "           0.5098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1098, 0.7804,\n",
              "           0.9882, 0.9882, 0.9922, 0.9882, 0.9882, 0.9137, 0.5686, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980,\n",
              "           0.5020, 0.9882, 0.9922, 0.9882, 0.5529, 0.1451, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 0)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks6plaIF-CWZ",
        "outputId": "07c3d2a1-ff8c-4b1e-d126-e17045907143"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
              "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
              "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
              "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
              "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
              "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
              "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
              "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
              "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
              "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
              "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
              "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
              "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
              "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
              "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
              "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
              "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for fold, (index_treino, index_teste) in enumerate(kfold.split(MNIST, target)):\n",
        "  print(f\"Fold {fold + 1} de {args['folds']}\")\n",
        "\n",
        "  treino_loader = DataLoader(MNIST_treino,\n",
        "                          batch_size = args['batch_size'],\n",
        "                          shuffle = True,\n",
        "                          num_workers = args['num_threads'])\n",
        "\n",
        "  teste_loader = DataLoader(MNIST_teste,\n",
        "                            batch_size = args['batch_size'],\n",
        "                            shuffle = True,\n",
        "                            num_workers = args['num_threads'])\n",
        "\n",
        "  executa(treino_loader, teste_loader, rede, args['num_epochs'])"
      ],
      "metadata": {
        "id": "R562HROsLbsN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48807c46-1aa8-4dda-ebe2-dc5ffb36fb1e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 de 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**********TREINA***********************\n",
            "Época 0, Loss: 2.3637 +\\- 0.1339, F1 Score: 0.0974, Tempo: 47.45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********VALIDA********************\n",
            "Época 0, Loss: 2.3629 +\\- 0.1328, F1 Score: 0.0983, Tempo: 5.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**********TREINA***********************\n",
            "Época 1, Loss: 2.3638 +\\- 0.1318, F1 Score: 0.0974, Tempo: 46.05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********VALIDA********************\n",
            "Época 1, Loss: 2.3630 +\\- 0.1305, F1 Score: 0.0982, Tempo: 6.03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**********TREINA***********************\n",
            "Época 2, Loss: 2.3638 +\\- 0.1324, F1 Score: 0.0974, Tempo: 46.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********VALIDA********************\n",
            "Época 2, Loss: 2.3630 +\\- 0.1334, F1 Score: 0.0982, Tempo: 4.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**********TREINA***********************\n",
            "Época 3, Loss: 2.3638 +\\- 0.1333, F1 Score: 0.0974, Tempo: 46.79\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********VALIDA********************\n",
            "Época 3, Loss: 2.3630 +\\- 0.1336, F1 Score: 0.0982, Tempo: 4.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**********TREINA***********************\n",
            "Época 4, Loss: 2.3638 +\\- 0.1331, F1 Score: 0.0974, Tempo: 46.33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********VALIDA********************\n",
            "Época 4, Loss: 2.3630 +\\- 0.1325, F1 Score: 0.0982, Tempo: 6.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**********TREINA***********************\n",
            "Época 5, Loss: 2.3638 +\\- 0.1327, F1 Score: 0.0974, Tempo: 46.17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********VALIDA********************\n",
            "Época 5, Loss: 2.3630 +\\- 0.1330, F1 Score: 0.0982, Tempo: 5.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**********TREINA***********************\n",
            "Época 6, Loss: 2.3638 +\\- 0.1335, F1 Score: 0.0974, Tempo: 50.21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********VALIDA********************\n",
            "Época 6, Loss: 2.3630 +\\- 0.1353, F1 Score: 0.0982, Tempo: 4.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "**********TREINA***********************\n",
            "Época 7, Loss: 2.3638 +\\- 0.1334, F1 Score: 0.0974, Tempo: 47.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********VALIDA********************\n",
            "Época 7, Loss: 2.3630 +\\- 0.1349, F1 Score: 0.0982, Tempo: 5.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-2df777f87d20>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                             num_workers = args['num_threads'])\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mexecuta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreino_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteste_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrede\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-93-5f3d4a87abce>\u001b[0m in \u001b[0;36mexecuta\u001b[0;34m(treino, teste, rede, epocas)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoca\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepocas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtreino_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreina\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtreino\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrede\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-7b09381e49c9>\u001b[0m in \u001b[0;36mtreina\u001b[0;34m(train_set, rede, epoca)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}