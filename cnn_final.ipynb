{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOsvzi5hbDL3fspTIYnQE3M"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aoivhMFGYq0F"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim, no_grad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from time import time\n",
        "from sklearn.metrics import f1_score as f1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MNIST_treino = datasets.MNIST('./', train = True, transform = transforms.ToTensor(), download=True)\n",
        "MNIST_teste =  datasets.MNIST('./', train = False, transform = transforms.ToTensor(), download=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp8rjYtXZBEJ",
        "outputId": "e3ad2841-4380-4a8d-f356-6a3eefb48558"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 107487716.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 43309150.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27007604.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7346906.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'batch_size':       5,    #quantidade de amostras por iteração\n",
        "    'num_threads':      4,    #número de threads do DataLoader\n",
        "    #número real de batches = 20 (tamanho de cada batch * quantidade de threads)\n",
        "    'num_classes':      len(MNIST_teste.classes),\n",
        "    'taxa_aprendizado': 1e-3, #\n",
        "    'weight_decay':     5e-3, #\n",
        "    'num_epochs':       30    #\n",
        "}\n",
        "\n",
        "neurons = {\n",
        "    'input_size':   1 * 28 * 28, #dimensão de entrada (imagens de 28x28 bits com 1 canal de cor. precisa ser achatado para uma única dimensão)\n",
        "    'hidden_size':  128, # dimensão escondida, hiperparâmetro\n",
        "    'out_size':     args['num_classes']\n",
        "}"
      ],
      "metadata": {
        "id": "c96F5SNvZS_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treino_loader = DataLoader(MNIST_treino,\n",
        "                          batch_size = args['batch_size'],\n",
        "                          shuffle = True,\n",
        "                          num_workers = args['num_threads'])\n",
        "\n",
        "teste_loader = DataLoader(MNIST_teste,\n",
        "                          batch_size = args['batch_size'],\n",
        "                          shuffle = True,\n",
        "                          num_workers = args['num_threads'])\n"
      ],
      "metadata": {
        "id": "4mM8oqROZYJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = nn.Sequential(\n",
        "        ## ConvBlock 1\n",
        "        nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),        # entrada: (b, 3, 32, 32) e saida: (b, 6, 28, 28)\n",
        "        nn.BatchNorm2d(6),\n",
        "        nn.ReLU(),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2, padding=0),           # entrada: (b, 6, 28, 28) e saida: (b, 6, 14, 14)\n",
        "\n",
        "        ## ConvBlock 2\n",
        "        nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),       # entrada: (b, 6, 14, 14) e saida: (b, 16, 10, 10)\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2, padding=0),           # entrada: (b, 16, 10, 10) e saida: (b, 16, 5, 5)\n",
        "\n",
        "        ## ConvBlock 3\n",
        "        nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0),     # entrada: (b, 16, 5, 5) e saida: (b, 120, 1, 1)\n",
        "        nn.BatchNorm2d(120),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),  # lineariza formando um vetor                # entrada: (b, 120, 1, 1) e saida: (b, 120*1*1) = (b, 120)\n",
        "\n",
        "        ## DenseBlock\n",
        "        nn.Linear(120, 84),                                         # entrada: (b, 120) e saida: (b, 84)\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(84, 10),                                          # entrada: (b, 84) e saida: (b, 10)\n",
        "        )\n",
        "\n",
        "# Subindo no hardware de GPU (se disponível)\n",
        "cnn = cnn.to('cuda')"
      ],
      "metadata": {
        "id": "5wjJYH8OZal4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(args['device'])\n",
        "optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
      ],
      "metadata": {
        "id": "cvDSHKZybKSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, net, epoch):\n",
        "\n",
        "  # Training mode\n",
        "  net.train()\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  epoch_loss  = []\n",
        "  pred_list, rotulo_list = [], []\n",
        "  for batch in train_loader:\n",
        "\n",
        "    dado, rotulo = batch\n",
        "\n",
        "    # Cast do dado na GPU\n",
        "    dado = dado.to(args['device'])\n",
        "    rotulo = rotulo.to(args['device'])\n",
        "\n",
        "    # Forward\n",
        "    ypred = net(dado)\n",
        "    loss = criterion(ypred, rotulo)\n",
        "    epoch_loss.append(loss.cpu().data)\n",
        "\n",
        "    _, pred = torch.max(ypred, axis=1)\n",
        "    pred_list.append(pred.cpu().numpy())\n",
        "    rotulo_list.append(rotulo.cpu().numpy())\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "  pred_list  = np.asarray(pred_list).ravel()\n",
        "  rotulo_list  = np.asarray(rotulo_list).ravel()\n",
        "\n",
        "  acc = accuracy_score(pred_list, rotulo_list)\n",
        "\n",
        "  end = time.time()\n",
        "  print('#################### Train ####################')\n",
        "  print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, end-start))\n",
        "\n",
        "  return epoch_loss.mean()\n",
        ""
      ],
      "metadata": {
        "id": "42I3tozobM7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(test_loader, net, epoch):\n",
        "\n",
        "  # Evaluation mode\n",
        "  net.eval()\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  epoch_loss  = []\n",
        "  pred_list, rotulo_list = [], []\n",
        "  with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "\n",
        "      dado, rotulo = batch\n",
        "\n",
        "      # Cast do dado na GPU\n",
        "      dado = dado.to(args['device'])\n",
        "      rotulo = rotulo.to(args['device'])\n",
        "\n",
        "      # Forward\n",
        "      ypred = net(dado)\n",
        "      loss = criterion(ypred, rotulo)\n",
        "      epoch_loss.append(loss.cpu().data)\n",
        "\n",
        "      _, pred = torch.max(ypred, axis=1)\n",
        "      pred_list.append(pred.cpu().numpy())\n",
        "      rotulo_list.append(rotulo.cpu().numpy())\n",
        "\n",
        "  epoch_loss = np.asarray(epoch_loss)\n",
        "  pred_list  = np.asarray(pred_list).ravel()\n",
        "  rotulo_list  = np.asarray(rotulo_list).ravel()\n",
        "\n",
        "  acc = accuracy_score(pred_list, rotulo_list)\n",
        "\n",
        "  end = time.time()\n",
        "  print('********** Validate **********')\n",
        "  print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Time: %.2f\\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, end-start))\n",
        "\n",
        "  return epoch_loss.mean()\n",
        ""
      ],
      "metadata": {
        "id": "C-98RTiFbVa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, test_losses = [], []\n",
        "for epoch in range(args['epoch_num']):\n",
        "\n",
        "  # Train\n",
        "  train_losses.append(train(train_loader, net, epoch))\n",
        "\n",
        "  # Validate\n",
        "  test_losses.append(validate(test_loader, net, epoch))"
      ],
      "metadata": {
        "id": "utzOeOYNbW0V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}